{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents <a id='top'></a>\n",
    "\n",
    "1. <a href=#eda>Data Exploration</a>\n",
    "1. <a href=#intro>Introduction to Regression</a>\n",
    "1. <a href=#resid>Residual Analysis</a>\n",
    "1. <a href=#simple>A Simple Model</a>\n",
    "1. <a href=#improvement>An Improvement</a>\n",
    "1. <a href=#ref>References and Links</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "# 1. Exploring Taiwan Dataset\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "For this tutorial, we shall work with a data set from the UCI machine learning repository. It contains real estate prices in the Xindian district of Taiwan. Our goal is to answer the following question:\n",
    "\n",
    "> How well can we explain real-estate prices in Taiwan?\n",
    "\n",
    "Let us first explore the dataset with Python. Make a few plots, and share what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re2 = pd.read_csv(\"../data/taiwan_dataset.csv\")\n",
    "re2.loc[:, 'trans_date':].describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(re2.loc[:, ['house_age', 'dist_MRT', 'num_stores', 'Xs', 'Ys', 'price']], figsize=(15, 15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "# 2. Introduction to Regression\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "## Model Definition\n",
    "\n",
    "A linear regression model relates a dependent variable ($y$) to several independent variables through an equation of the form:\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\cdots + \\beta_{p-1} x_{p-1,i} + \\epsilon_i $$\n",
    "\n",
    "The $y$ and $x$ are observed, but the $\\beta$'s and the $\\epsilon$'s are not. The index $i$ refers to a particular observation. Let us assume we have $n$ observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation\n",
    "\n",
    "To estimate the $\\beta$ values, we minimise a cost function. One of the most commonly used ones is the least squares objective function:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_{1,i} - \\beta_2 x_{2,i} - \\ldots \\beta_{p-1} x_{p-1,i})^2\n",
    "$$\n",
    "\n",
    "We denote the optimal estimates as $\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_{p-1}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals\n",
    "\n",
    "Once we have the $\\hat{\\beta}$'s, we can compute **fitted values** by applying the equation back to the observed $x$ values. We can then compute **residuals**. There will be $n$ residuals, one for each observation that we have. We are going to use the residuals to help improve our model fit.\n",
    "\n",
    "$$\n",
    "\\hat{y_i} =  \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1,i} + \\hat{\\beta}_2 x_{2,i} +\n",
    "\\cdots + \\hat{\\beta}_{p-1} x_{p-1,i}\n",
    "$$ \n",
    "\n",
    "Residuals are defined to be \n",
    "$$\n",
    "r_i = y_i - \\hat{y_i}\n",
    "$$\n",
    "\n",
    "Here is what me mean, in a **simple** linear regression setting. The red values are the fitted values.\n",
    "\n",
    "<img src=\"../figs/residuals-1.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='resid'></a>\n",
    "# 3. Residual Analysis\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "## Influential Points\n",
    "\n",
    "Some points in our dataset will influence the coefficients, and thus fitted values, and thus the residuals more than others. We need to identify these and study them, because we do not want our model driven by these chance occurences or wild points. Typically, influential points have one or both these properties:\n",
    "\n",
    "1. They may be far away from their neighbours in the predictor space.\n",
    "2. They may have an unduly large residual.\n",
    "\n",
    "### On Coefficients\n",
    "\n",
    "To see if point $i$ is influential on coefficient $j$, we \n",
    "\n",
    "* Estimate the model coefficients with all the data points.\n",
    "* Leave out the observations $(x_i, y_i)$ one at a time and re-estimate the model coefficients.\n",
    "* Compare the $\\beta_j$'s ($n$ of them) from step 2 with the original estimate from step 1.  \n",
    "\n",
    "<img src=\"../figs/inf_coef-1.png\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Fitted Values\n",
    "\n",
    "To assess the influence of each point on the fitted values, we use Cook's Distance, which is given by this formula:\n",
    "\n",
    "$$\n",
    "D_i = \\frac{\\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j,-i})^2}{p s^2}$$\n",
    "\n",
    "where \n",
    "*   $\\hat{y}_{j,-i}$ is the fitted value for $y_j$ using the model that is fitted without observation $i$.\n",
    "* $p$ is the number of predictors, including the intercept. In the diagrams above, we are considering only one predictor $x$ for the response variable $y$, and thus call it \"simple\" as opposed to \"multiple\" linear regression. Hence $p=2$.\n",
    "* $s^2$ is a measure of the variance of $\\epsilon$; it is computed as\n",
    "$$ \n",
    "  s^2 = \\frac{1}{n-p} \\sum_{i=1}^n r_i^2\n",
    "$$\n",
    "\n",
    "We typically set aside and investigate the 3 - 4 most influential points at each iteration of our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Plots\n",
    "\n",
    "We plot residuals against\n",
    "* fitted values.\n",
    "* explanatory variables one at a time.\n",
    "\n",
    "\n",
    "<img src=\"../figs/residual_patterns-1.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Residual Plots\n",
    "\n",
    "Other plots that could (and should) be made are\n",
    "* Residuals against time order of data collection.\n",
    "* Residuals against new variables to be considered for addition into the model.\n",
    "* Residuals against a product of existing variables to check for interaction.\n",
    "* QQ plots\n",
    "\n",
    "<img src=\"../figs/qq_ex_1-1.png\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Model Fit\n",
    "\n",
    "We can compute the $R^2$ of a model to assess how well it explains the variability in the observed values.\n",
    "\n",
    "$$ \n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^n r^2_i}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "A value close to 1 is better than a value close to 0. However, the $R^2$ can be misleading since it can be artificially increased by adding more predictors. Hence we usually look at the adjusted $R^2$, which penalises a model for using too many explanatory variables.\n",
    "\n",
    "$$ \\text{adjusted } R^2 = 1 - \\frac{\\frac{1}{n-1-p}\\sum_{i=1}^n\n",
    "r^2_i}{\\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})^2}$$\n",
    "\n",
    "Another statistic I tend to compute is the Predictive Residual Sum of Squares. It yields some indication of how well the model can generalise.\n",
    "\n",
    "  $$ \\sum_{i=1}^n (\\hat{y}_{-i} - y_i)^2 $$\n",
    "  \n",
    "To get this quantity back to the scale of the original \\(y\\)-variable, we compute the root mean squared prediction error: \n",
    "$$ \n",
    "  \\text{RMSPE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (\\hat{y}_{-i} - y_i)^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='simple'></a>\n",
    "# 4. A Simple Model\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "Let's begin with a simple model:\n",
    "$$\n",
    "\\text{price} = \\beta_0 + \\beta_1 \\text{age} + \\beta_2 \\text{dist} + \n",
    "\\beta_3 \\text{(num. of stores)}  + \\beta_4 \\text{X} + \\beta_5 \\text{Y} + \\epsilon\n",
    "$$\n",
    "\n",
    "Note that $p=6$ in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.outliers_influence as oinf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = re2[['house_age', 'dist_MRT', 'num_stores', 'Xs', 'Ys']]\n",
    "y = re2.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_model = sm.OLS(y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_results = s_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjusted $R^2$ is approximately 0.57, but there is a warning about high condition number and numerical instability. If we look at the coefficients, the MRT distance is very small yet highly significant. The scale of the MRT distance is too large compared to the rest. This is probably causing the matrix inversion to be unstable. Additionally, the coefficient for the X-variable does not seem to be significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_results_inf = s_results.get_influence()\n",
    "r_star = s_results_inf.get_resid_studentized_external()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSPE = np.sqrt((s_results_inf.resid_press**2).mean())\n",
    "\n",
    "RMSPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a panel of residual plots. Take a close look at them, and identify which points you should investigate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(2, 3, squeeze=False, figsize=(14,8))\n",
    "\n",
    "for i,cc in enumerate(X.columns[1:]):\n",
    "    plt.subplot(2,3, i+1)\n",
    "    plt.scatter(X[cc], r_star, alpha=0.8)\n",
    "    plt.title(cc)\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.scatter(s_results.fittedvalues, r_star, alpha=0.8)\n",
    "plt.title('Fitted Values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here are the differences in beta coefficient values, along with the Cook's distance plot. There is one point that is very influential. Isolate this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_results_inf.dfbetas[:5, :].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(131)\n",
    "plt.scatter(np.arange(414), s_results_inf.dfbetas[:,2]);plt.title('betadiff: MRT');plt.xlabel('id');\n",
    "plt.subplot(132)\n",
    "plt.scatter(np.arange(414), s_results_inf.dfbetas[:,3]);plt.title('betadiff: num. stores');plt.xlabel('id');\n",
    "plt.subplot(133)\n",
    "plt.scatter(np.arange(414), s_results_inf.cooks_distance[0]);plt.title('Cooks Distance');plt.xlabel('id');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.inference import check_normality as cn\n",
    "\n",
    "cn(pd.Series(r_star))\n",
    "#oinf.variance_inflation_factor(X.iloc[:,1:].to_numpy(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the things we can try to remedy:\n",
    "\n",
    "1. distance should be transformed.\n",
    "2. There is still a little remaining curvature in age.\n",
    "3. There is one outlier that is also influential (on fits and on the coefficients).\n",
    "4. X is not significant.\n",
    "5. Is the residuals vs. fitted indicating right-skewness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='improvement'></a>\n",
    "# 5. An Improvement\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "Let's try two things first, to see if we can improve the adjusted $R^2$: take a log transform of the distance, and drop the influential point (point number 271)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = re2.loc[:, ['house_age', 'num_stores', 'Xs', 'Ys']]\n",
    "X2.loc[:, 'ldist'] = np.log(X.dist_MRT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y.drop(270)\n",
    "X2 = X2.drop(index=270)\n",
    "\n",
    "X2 = sm.add_constant(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_model = sm.OLS(y2, X2)\n",
    "s2_results = s2_model.fit()\n",
    "s2_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning about numerical instability disappears, the coefficient for distance is significant and the adjusted $R^2$ has gone up. Take some time to do the following, and compare it to the earlier model:\n",
    "\n",
    "1. Residual plots\n",
    "2. RMSPE statistic\n",
    "\n",
    "Are we doing better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "# 6. References\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "1. [statsmodels API](https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html#examples-notebooks-generated-ols--page-root) The documentation is not as good as sklearn, but still decent.\n",
    "2. Applied Regression Analysis, by Draper and Smith."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
