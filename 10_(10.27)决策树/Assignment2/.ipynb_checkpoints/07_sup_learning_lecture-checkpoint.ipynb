{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents <a id='top'></a>\n",
    "\n",
    "1. <a href=#eda>Data Exploration</a>\n",
    "1. <a href=#sup>Supervised Learning</a>\n",
    "1. <a href=#skl>sklearn</a>\n",
    "1. <a href=#metrics>Classification Error Metrics</a>\n",
    "1. <a href=#app>Application</a>\n",
    "    1. <a href=#knn>k-Nearest Neighbours</a>\n",
    "    1. <a href=#hyper>Hyperparameter Search</a>\n",
    "1. <a href=#ref>References and Links</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "# 1. Introduction to Lending Club Data\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "The data set we will be using comes from the Lending Club. It is a peer-to-peer lending company. It offers loans that are funded by other people: \n",
    "\n",
    "* A borrower applies for a loan of a certain amount.\n",
    "* The company assesses the risk of lending. \n",
    "* Even if an application is accepted, the requested loan might not be fully funded by investors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full dataset can be obtained from the [Kaggle website](https://www.kaggle.com/wordsforthewise/lending-club). It is approximately 650MB in size. However, for our session, we are only going to work with a partial dataset, from 2007 to 2011. It is available on LumiNUS as `loans.xlsx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = pd.read_excel('../data/loans.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans[['loan_amnt', 'funded_amnt_inv', 'y']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our version of the loans dataset consists of 42,535 rows and 76 columns. The Kaggle site contains a data dictionary that explains what each column means. Our target variable is contained in the column `y`: we wish to predict it using the remaining columns. \n",
    "\n",
    "It was computed from the existing columns as follows. First, for each loan (i.e. row), we compute the proportion of loan not funded:\n",
    "\n",
    "$$\n",
    "\\text{prop. not funded} = \\frac{(\\text{loan amnt}) - (\\text{funded amnt inv})}{\\text{loan amnt}}\n",
    "$$\n",
    "\n",
    "Our target variable takes on the value 1 if the proportion not funded is larger than 0.05:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "1 & \\text{if prop. not funded} \\ge 0.05 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, only about 19% of the loans were unsuccessful. This indicates a moderately unbalanced dataset, and serves as our benchmark. If we were to always predict that a loan is successful (y=0), then we would be correct approximately 81% of the time.\n",
    "\n",
    "> *Can we do better than this?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sup'></a>\n",
    "# 2. Supervised Learning\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "In supervised learning, our goal is to develop a model that can predict a quantity of interest from a set of features. In this process,\n",
    "\n",
    "* Algorithms learn from a training set of labelled examples.\n",
    "* This training set is meant to be representative of the set of all possible inputs.\n",
    "* Example algorithms include logistic regression, support vector machines, decision trees and random forests.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "1. We wish to predict if a student will graduate from university or not, based on his/her 'A' level results.\n",
    "2. We wish to predict tomorrow's stock price based on today's price.\n",
    "\n",
    "The other main sort of learning is unsupervised learning. Here are some examples:\n",
    "\n",
    "1. We wish to identify the salient topics from a set of English documents.\n",
    "2. We wish to estimate the probability density function that a sample of observations came from.\n",
    "\n",
    "In our class, we shall focus only on *supervised* learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification versus Regression\n",
    "\n",
    "If the answer to the question (supervised learning problem) we are facing is either YES or NO, then we have a **classification** problem.\n",
    "\n",
    "* Given the results of a clinical test, does this patient suffer from diabetes?\n",
    "* Given an MRI, is there a tumor?\n",
    "\n",
    "On the other hand, if we are trying to predict a real-valued quantity,\n",
    "then we are faced with a **regression** problem.\n",
    "\n",
    "* Given the details about an apartment, what will the rental be?\n",
    "* Given historical transactions of a customer, how much is he likely to spend on his next purchase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Overview\n",
    "\n",
    "<img src=\"../figs/sup_learning.png\" style=\"width: 900px;\"/>\n",
    "\n",
    "In words:\n",
    "\n",
    "1. Split up a dataset into training and testing data. **Do not touch the test data again until the end.**\n",
    "2. Preprocess/clean the training data and store the parameters for later use on the test data.\n",
    "    * Example preprocessings are scaling, one-hot encoding, PC \n",
    "    decomposition, etc.\n",
    "3. Decide on what models you wish to try. Each model has parameters to be fit (from the data), and **hyperparameters** to be chosen by you.\n",
    "    * Example models are k-nearest neighbours (KNN) and random forests.\n",
    "    * A hyperparameter for KNN is the number of neighbours to use.\n",
    "    * A hyperparameter for random forests is the number of trees.\n",
    "    * Hyperparameters usually control the **complexity** of a \n",
    "    model. If a model is too complex, it will over-fit to the \n",
    "    training data but fare poorly on the test data.\n",
    "4. Use **cross-validation** or a set-aside validation set to decide on the hyperparameters for your chosen estimator. These procedures typically minimise a loss function or error metric.\n",
    "5. Once you are satisfied with your choices, evaluate the selected model on the test set to obtain an estimate of your generalisation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='skl'></a>\n",
    "# 3. Scikit-learn\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "* Scikit-learn is a library in Python which has several useful functions used in machine learning.\n",
    "* The library has many algorithms for classification, regression, clustering and other machine learning methods.\n",
    "* It uses other libraries like NumPy and matplotlib which are also\n",
    "used in this course.\n",
    "* The website for scikit-learn is an excellent source of examples and tips on using the functions within this package:\n",
    "http://scikit-learn.org/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All objects in scikit-learn have common access points. The three main \n",
    "interfaces are:\n",
    "\n",
    "1. Estimator interface - `fit()` method.\n",
    "    * This function allows us to build and fit models.\n",
    "    * Any object that can estimate some parameters based on a dataset \n",
    "    is an *estimator*.\n",
    "\t* Estimation is performed by the `fit()` method. This method \n",
    "    takes in two datasets as arguments (the input data, and the\n",
    "    corresponding output/labels).\n",
    "    \n",
    "2. Predictor interface - `predict()` method.\n",
    "    * This function allows us to make predictions.\n",
    "\t* Estimators capable of making predictions when given a \n",
    "    dataset are called *predictors*.\n",
    "\t* A predictor has a `predict()` method. It takes in a dataset \n",
    "    of new instances and returns a dataset of corresponding \n",
    "    predictions.\n",
    "    \n",
    "3. Transformer interface - `transform()` method.\n",
    "    * This function is for converting data.\n",
    "\t* Estimators which can also transform a dataset are called *transformers*.\n",
    "\t* Transformations are carried out by the `transform()` method.\n",
    "    * This method takes in the dataset to transform as a parameter and\n",
    "    returns the transformed dataset.\n",
    "    * We will not have too much time to spend on the transformer \n",
    "    interface in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data Structure\n",
    "\n",
    "For supervised learning problems in scikit-learn, the input data has to be structured in NumPy-like arrays.\n",
    "\n",
    "\n",
    "The **feature matrix X**, of shape $n$ by $d$ contains features:\n",
    "* $n$ rows: the number of samples\n",
    "* $d$ columns: the number of features or distinct traits used to\n",
    "describe each item in a quantitative manner\n",
    "\n",
    "Each row in the feature matrix is referred to as a sample, example or an instance.\n",
    "\n",
    "A **label vector y** stores the target values. This vector stores the true output value for each corresponding sample (row) in\n",
    "matrix X.\n",
    "\n",
    "<img src=\"../figs/05_input_structure.png\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='metrics'></a>\n",
    "# 4. Measures of Performance\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "Before we head into creating classifiers which will help us predict if a loan will be successful, let's understand what determines the usefulness of a classifier.\n",
    "\n",
    "A basic measure of performance would be the *accuracy* of predictions.\n",
    "\n",
    "$$ \\text{accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When more detailed analysis is needed, partial performance metrics can be presented in a *confusion matrix*.\n",
    "\n",
    "It considers various scenarios depending on the classifier's prediction and the actual outcome.\n",
    "\n",
    "<img src=\"../figs/day_07_confusion.png\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP, FP, TN, FN\n",
    "\n",
    "In the confusion matrix, there are 4 possible cases:\n",
    "* True positives (TP)\n",
    "    * Classifier predicts sample as positive and it really is so.\n",
    "* False positives (FP)\n",
    "    * Classifier predicts sample as positive but in truth, it is \n",
    "    negative.\n",
    "    * An inaccurate prediction.\n",
    "* True negatives (TN)\n",
    "    * Classifier predicts sample as negative and it really is so.\n",
    "* False negatives (FN)\n",
    "    * Classifier predicts sample as negative but in truth, it is \n",
    "    positive.\n",
    "    * An inaccurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "With the confusion matrix, more performance metrics can be defined besides the accuracy of a classifier.\n",
    "\n",
    "* The **recall** of a classifier is the proportion of truly positives correctly\n",
    "identified:\n",
    "$$\n",
    "\\text{recall}=  \\frac{\\text{TP}}{\\text{TP + FN}} $$\n",
    "* The **precision** of a classifier is the proportion of predicted\n",
    "positives that are truly positive:\n",
    "$$\n",
    "\\text{precision} =  \\frac{\\text{TP}}{\\text{TP + FP}} $$\n",
    "* Above, we have defined recall and precision for the *positive* category outcome. There are analogous definitions for the *negative* outcome.\n",
    "* Recall is also referred to as the True Positive Rate (TPR). One\n",
    "minus the precision is also referred to as the False Positive Rate (FPR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 score\n",
    "\n",
    "* The harmonic mean of two numbers $x_1$ and $x_2$ is \n",
    "$$\n",
    "\\left( \\frac{1/x_1 + 1/x_2}{2} \\right)^{-1}\n",
    "$$\n",
    "* We can combine precision and recall into one score using their harmonic mean:\n",
    "$$\n",
    "\\text{F1} = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} +\n",
    "\\text{receall}}\n",
    "$$\n",
    "Roughly, the F1 score is a summary of how good the classifier is in terms of both precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='app'></a>\n",
    "# 4. Application to Lending Club Data\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "## Pre-processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, metrics, preprocessing\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, validation_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, f1_score, precision_recall_curve\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "#from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.loc[:, 'issue_yr'] = loans.issue_d.apply(lambda x: x.year)\n",
    "loans.loc[:, 'issue_mth'] = loans.issue_d.apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we drop those columns that have fewer than 40000 non-missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_these_columns = loans.apply(lambda x: np.sum(pd.notna(x)), axis=0) < 40000\n",
    "drop_these_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.drop(columns=loans.columns[drop_these_columns], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we drop all rows that have even 1 missing value. A better way would be to impute the missing values, but we save that for another time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values rows\n",
    "no_miss = loans[pd.notna(loans).all(axis=1)].copy()\n",
    "no_miss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_miss.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of columns that contain date information in our dataset: `issue_d` and `earliest_cr_line`. Let us extract the year information from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_miss.loc[:, 'issue_yr'] = no_miss.issue_d.apply(lambda x: x.year)\n",
    "#no_miss.loc[:, 'issue_mth'] = no_miss.issue_d.apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_miss.earliest_cr_line.str.split('-', expand=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_line_cols = no_miss.earliest_cr_line.str.split('-', expand=True)\n",
    "cr_line_cols.columns = ['ecrl_mth', 'ecrl_yr']\n",
    "cr_line_cols.ecrl_yr = cr_line_cols.ecrl_yr.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_miss = pd.concat([no_miss, cr_line_cols],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_miss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split the data into a training and test set before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = no_miss.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(no_miss, y, test_size=0.3, random_state=41, \n",
    "                                                 stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding useful Features\n",
    "\n",
    "In this section, we whittle down the number of features we have in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X_train.loc[:, ['loan_amnt', 'int_rate', 'installment', \n",
    "                   'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', \n",
    "                   'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', \n",
    "                   'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', \n",
    "                   'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', \n",
    "                   'collection_recovery_fee', 'collections_12_mths_ex_med',\n",
    "                   'acc_now_delinq', 'issue_yr', 'issue_mth', 'ecrl_yr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skb = SelectKBest(mutual_info_classif, k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skb.fit(X_num, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skb.scores_.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_num.columns[skb.scores_ >= np.sort(skb.scores_)[-8]]\n",
    "#X_num.columns[np.argsort(-skb.scores_)[:8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "skb.transform(X_num).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num.loc[:, X_num.columns[skb.scores_ >= np.sort(skb.scores_)[-8]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a similar thing for the categorical columns, using the following code. None of them were clear, so I kept all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_df = X_train.loc[:,['term', 'grade', 'sub_grade', 'emp_length',\n",
    "       'home_ownership', 'verification_status', \n",
    "       'loan_status', 'purpose', 'zip_code','addr_state'] ]\n",
    "X_cat_df = X_cat_df.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe1 = OrdinalEncoder()\n",
    "oe1.fit(X_cat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = oe1.transform(X_cat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_cat[0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe1.inverse_transform(X_cat[:2,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe1.categories_[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skb2 = SelectKBest(mutual_info_classif, k = 10)\n",
    "#skb2a = SelectKBest(chi2, k = 10)\n",
    "\n",
    "skb2.fit(X_cat, y_train)\n",
    "#skb2a.fit(X_cat, y_train)\n",
    "\n",
    "#X_cat_df.columns[np.argsort(skb2a.scores_)]\n",
    "X_cat_df.columns[np.argsort(skb2.scores_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following chosen numerical and categorical features, we scale the numerical and one-hot encode the categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['loan_amnt', 'int_rate', 'installment', 'total_pymnt', \n",
    "                'total_pymnt_inv', 'total_rec_prncp', 'issue_yr']\n",
    "cat_features = ['term', 'grade', 'emp_length', 'home_ownership', \n",
    "                'loan_status', 'purpose', 'addr_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = num_features + cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.loc[:, all_features]\n",
    "X_test = X_test.loc[:, all_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the scaling and encoding at one go, and because we need to store the fitted parameters for later use on the test set, we use a pipeline of transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([\n",
    "      ('scale', StandardScaler(),\n",
    "      make_column_selector(dtype_include=np.number)),\n",
    "      ('onehot', OneHotEncoder(),\n",
    "      make_column_selector(dtype_include=object))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ttrain = ct.transform(X_train)\n",
    "X_ttest = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[:10, :7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ttrain[:10, :7].toarray().round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>\n",
    "## k-Nearest Neighbours\n",
    "\n",
    "k-Nearest Neighbours (KNN) is a simple model that tries to classify a set of data points into groups.\n",
    "\n",
    "* The *k* in KNN refers to the number of nearest data points the algorithm should include before classifying them into a group (i.e. number of neighbours).\n",
    "    * This a parameter you get to set when using `KNeighborsClassifiers()`\n",
    "* We start with a single data point in the picture. Each time a new data point is added, its *k* closest neighbours (data points) are identified.\n",
    "* Note that the definition of 'nearest' is subjective; we can choose the metric appropriate for the situation.\n",
    "* Since its neighbours have already been classified into different groups, the new data point will be added to the group which the majority of the neighbours are in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we added a new data point *c* with *k = 3*, it is grouped into *b*, since two out of three points in the neighbourhood belong to *b*.\n",
    "\n",
    "<img src=\"../figs/05_knn.png\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn10 = neighbors.KNeighborsClassifier(n_neighbors=10, n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn10.fit(X_ttrain, y_train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = nn10.predict(X_ttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hyper'></a>\n",
    "## Grid Search for Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn_range= np.arange(10, 1, -1)\n",
    "nn_range = np.arange(50, 5, -4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "test1 = GridSearchCV(neighbors.KNeighborsClassifier(), {'n_neighbors':nn_range[:2]}, \n",
    "                     scoring='f1', cv=5, verbose=2)\n",
    "\n",
    "test1.fit(X_ttrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_scores, cv_scores = validation_curve(neighbors.KNeighborsClassifier(), X_ttrain, y_train, \n",
    "                                           param_name='n_neighbors', cv=5, n_jobs=8,\n",
    "                                           param_range=nn_range, scoring='f1', verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_means = np.mean(train_scores, axis=1)\n",
    "train_sd = np.std(train_scores, axis=1)\n",
    "\n",
    "cv_means = np.mean(cv_scores, axis=1)\n",
    "cv_sd = np.std(cv_scores, axis=1)\n",
    "\n",
    "plt.plot(1/nn_range, train_means, 'o-', label='Training', color='blue')\n",
    "plt.fill_between(1/nn_range, train_means-train_sd, train_means+train_sd, color='blue', alpha=0.2)\n",
    "\n",
    "plt.plot(1/nn_range, cv_means, 'o-', label='CV (Test)', color='red')\n",
    "plt.fill_between(1/nn_range, cv_means-cv_sd, cv_means+cv_sd, color='red', alpha=0.2)\n",
    "\n",
    "plt.legend(loc='lower right');plt.ylabel('F1-score');plt.xlabel('Complexity');plt.title('Validation Curve');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ttrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves and Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn20 = neighbors.KNeighborsClassifier(n_neighbors=20, n_jobs=8)\n",
    "nn20.fit(X_ttrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_probs = nn20.predict_proba(X_ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = nn20.predict(X_ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(y_test, y_test_probs[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr,'b-');\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('FPR');plt.ylabel('TPR');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The area under the AUC is {auc(fpr, tpr):.3f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_precision, nn_recall, thresholds = precision_recall_curve(y_test, y_test_probs[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(nn_recall, nn_precision,'b-');\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall');plt.ylabel('Precision');\n",
    "plt.plot([0.0, 1.0], [0.0, 1.0], color='red', linestyle=\"--\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'prec':nn_precision[1:], 'rec':nn_recall[1:], 'thresh':thresholds})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "# 6. References\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "1. [Introduction to statistical learning](https://faculty.marshall.usc.edu/gareth-james/ISL/) This is one of the most complete introductory books on the topic.\n",
    "2. [scikit-learn docs](https://scikit-learn.org/stable/user_guide.html)\n",
    "3. Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems By Aurélien Géron (electronic copy available at NUS libraries)\n",
    "4. Introduction to Data Science A Python Approach to Concepts, Techniques and Applications by Laura Igual (electronic version available at NUS libraries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
