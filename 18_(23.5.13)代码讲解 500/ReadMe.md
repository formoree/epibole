### 早上

1. google翻译这一章
2. 代码做一些笔记

### 文章摘要

我们仍然遵循具有跳过连接的UNet的5层深度编码器-解码器架构，但改变了每个块的设计。我们在UNeXt中有两个阶段——一个卷积阶段，然后是MLP阶段。我们在网络的初始和最终块中使用具有较少滤波器数量的卷积块。在瓶颈中，我们使用了一种新的令牌化MLP（Tok-MLP）块，它能够有效地保持较少的计算量，同时能够对良好的表示进行建模。标记化MLP将卷积特征投影到抽象标记中，然后使用MLP学习有意义的信息进行分割。我们还在MLP中引入移位操作，以提取与不同轴向移位相对应的局部信息。由于标记化特征的维数较小，MLP的复杂度也比卷积或自注意和变换器低；

### 2 UXNET

网络设计：UNeXt 是一种编码器-解码器架构，具有两个阶段：1) 卷积阶段和 2) 标记化 MLP 阶段。 输入图像通过编码器，其中前 3 个块是卷积块，接下来的 2 个是标记化 MLP 块。 解码器有 2 个标记化 MLP 块，后跟 3 个卷积块。 每个编码器块将特征分辨率降低 2，每个解码器块将特征分辨率增加 2。编码器和解码器之间也包含跳过连接。 每个块的通道数是一个超参数，表示为 C1 到 C5。 对于使用 UNeXt 架构的实验，除非另有说明，否则我们遵循 C1 = 32、C2 = 64、C3 = 128、C4 = 160 和 C5 = 256。 请注意，这些数字实际上小于 UNet 及其变体的过滤器数量，这些过滤器有助于第一次更改以减少参数和计算

卷积阶段：每个卷积块都配备了一个卷积层、一个批量归一化层和 ReLU 激活层。 我们使用 3 × 3 的内核大小，1 的步幅和 1 的填充。编码器中的转换块使用池窗口为 2 × 2 的最大池化层，而解码器中的转换块由双线性插值层组成以进行上采样 特征图。 **我们使用双线性插值而不是转置卷积，因为转置卷积基本上是可学习的上采样并有助于提供更多可学习的参数。** 

Shifted MLP：在 shifted MLP 中，**我们首先在标记化之前移动转换特征通道的轴。 这有助于 MLP 仅关注转换特征的某些位置，从而将局部性引入块。** 这里的直觉类似于 Swin transformer [5]，其中引入了基于窗口的注意力来为一个完全全局的模型添加更多的局部性。 由于 Tokenized MLP 块有 2 个 MLP，我们将一个特征跨宽度移动，另一个跨高度移动，就像在轴向注意 [24] 中一样。 我们将特征分成 h 个不同的分区，并根据指定的轴将它们移动 j 个位置。 这有助于我们创建随机窗口，沿轴引入局部性。

Tokenized MLP Stage：在 tokenized MLP 块中，我们首先移动特征并将它们投影到 token 中。 为了标记化，我们首先使用 3 的内核大小并将通道数更改为 E，其中 E 是嵌入维度（标记数），它是一个超参数。 然后，我们将这些标记传递给移位的 MLP（跨宽度），其中 MLP 的隐藏维度是超参数 H。接下来，特征通过深度卷积层 (DW-Conv)。 **我们在此块中使用 DWConv 有两个原因：1) 它有助于编码 MLP 特征的位置信息。 [26] 表明，MLP 块中的 Conv 层足以对位置信息进行编码，并且它实际上比标准位置编码技术表现更好。 当测试和训练分辨率不相同时，需要对 ViT 中的位置编码技术进行插值，这通常会导致性能下降。 2) DWConv 使用较少数量的参数，因此提高了效率。** 然后我们使用 GELU [12] 激活层。 我们使用 GELU 而不是 RELU，因为它是一种**更平滑的替代方案，并且被发现性能更好**。 此外，最近的架构如 ViT [10] 和 BERT [9] 已成功使用 GELU 获得改进的结果。 然后，我们将这些特征传递给另一个将维度从 H 转换为 O 的移位 MLP（跨高度）。我们在这里使用残差连接并将原始标记添加为残差。 然后我们应用层归一化 (LN) 并将输出特征传递到下一个块。 LN 优于 BN，因为它更有意义地沿着令牌进行规范化，而不是在令牌化 MLP 块中跨批次进行规范化。 Tokenized MLP 块中的计算可以概括为：其中 T 表示标记，H 表示高度，W 表示宽度，DW Conv 表示深度卷积，LN 表示层归一化。 请注意，所有这些计算都是在嵌入维度 H 上执行的，该维度明显小于特征映射 H N × H N 的维度，其中 N 是 2 的因数，具体取决于块。 在我们的实验中，除非另有说明，否则我们将 H 设置为 768。 这种设计标记化 MLP 块的方式有助于编码有意义的特征信息，并且在计算或参数方面贡献不大

### 代码

Image to Patch Embedding是一种将图像转化为补丁（patch）表示的方法，常用于将图像输入到Transformer等深度学习模型中。其主要思想是将图像分成若干个大小相等的补丁，并将每个补丁视为一个向量，以此来表示整个图像。这样做的好处是可以将图像的空间信息转化为向量，方便深度学习模型进行处理。

双线性插值层是深度学习领域中常用的一种图像处理技术，用于对图像进行放缩操作。它可以根据已知的像素值，在两个方向上进行线性插值，得到新的像素值，从而实现图像的放缩。